{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dffec3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.environ[\"USERPROFILE\"]+\"/Desktop/Jupyter-GPU/data/nasa_planet_gan/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a25f7140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display as ipydisplay\n",
    "from PIL import Image\n",
    "from importlib import reload\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#import keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.activations as activations\n",
    "import loadfunctions as lf\n",
    "import tf_gan as tfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "256d42ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_width = 128\n",
    "input_height = int(input_width*.75)\n",
    "input_channels = 3\n",
    "num_examples_to_generate = 16\n",
    "noise_dim = 64\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dde3e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'tf_gan' from 'C:\\\\Users\\\\Tasha\\\\Desktop\\\\Jupyter-GPU\\\\data\\\\nasa_planet_gan\\\\tf_gan.py'>"
     ]
    }
   ],
   "source": [
    "reload(lf)\n",
    "reload(tfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d075818d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tfg.create_batch(input_height,input_width, 40)\n",
    "#print(batch.shape)\n",
    "#grid = tfg.create_image_grid(batch,16)\n",
    "#tfg.get_image(grid).show()\n",
    "#for i in range(0,180,10):    \n",
    "#    ex1 = batch[0]\n",
    "#    ex1 = lf._rotate_and_crop(ex1,input_height,input_width,i)\n",
    "#    try:\n",
    "#        ex1=ex1.numpy()\n",
    "#    except:\n",
    "#        pass\n",
    "#    tfg.get_image(ex1).show()\n",
    "#\n",
    "\n",
    "#ex2 = lf._rotate_and_crop(ex1.numpy().copy(),input_height,input_width,35)\n",
    "#get_image(ex2.numpy()).show()\n",
    "\n",
    "#get_image(batch[-1]).show()\n",
    "#get_image(batch[-2]).show()\n",
    "\n",
    "#tf.image.resize\n",
    "#batch = create_batch(2,2)\n",
    "#tfg.get_image(batch[-0]).show()\n",
    "#tfg.get_image(batch[-1]).show()\n",
    "#tfg.get_image(batch[-2]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be23ee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipydisplay.display(\n",
    "tf.keras.preprocessing.image.array_to_img(batch[\n",
    "    random.randint(0,len(batch))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2adbe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train= batch\n",
    "\n",
    "np.random.shuffle(X_train)\n",
    "ipydisplay.display(tf.keras.preprocessing.image.array_to_img(X_train[0]))\n",
    "\n",
    "test_num = int(len(X_train)*.1)\n",
    "X = X_train[0:-test_num]\n",
    "X_test = X_train[-test_num:]\n",
    "\n",
    "print(f\"X_train: {X.shape}, X_test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f7d07f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to provide the generator of the GAN\n",
    "from tensorflow.keras.layers import BatchNormalization,Dense,Activation\n",
    "\n",
    "def generator_model(output_width,\n",
    "                    output_height,\n",
    "                    output_channels,\n",
    "                    latent_dim,\n",
    "                    filters_list=[512,256,64,32,16,8,4,2],\n",
    "                    dtype=tf.float32):\n",
    "    w8 = int(output_width/8)\n",
    "    h8 = int(output_height/8)\n",
    "    con_t_layers=filters_list\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=(latent_dim)))\n",
    "    \n",
    "    model.add(Dense(units=int(np.ceil(latent_dim/192)*192),dtype=dtype ))\n",
    "    model.add(BatchNormalization(dtype=dtype))\n",
    "    model.add(Activation(activations.tanh,dtype=dtype))\n",
    "    \n",
    "    model.add(Dense(units=int(np.ceil(latent_dim/192)*192) ,dtype=dtype))\n",
    "    model.add(BatchNormalization(dtype=dtype))\n",
    "    model.add(Activation(activations.tanh,dtype=dtype))\n",
    "    \n",
    "    model.add(Dense(units=int(np.ceil(latent_dim/192)*192),dtype=dtype))\n",
    "    model.add(BatchNormalization(dtype=dtype))\n",
    "    model.add(Activation(activations.tanh,dtype=dtype))\n",
    "\n",
    "    model.add(tf.keras.layers.Reshape(target_shape=(h8,w8,-1)))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2DTranspose(filters=con_t_layers[0],kernel_size=(5,5),strides=(2,2),padding='same',dtype=dtype))\n",
    "    model.add(tf.keras.layers.BatchNormalization(dtype=dtype))\n",
    "    model.add(Activation(activations.tanh,dtype=dtype))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2DTranspose(filters=con_t_layers[1],kernel_size=(5,5),strides=(2,2),padding='same',dtype=dtype))\n",
    "    model.add(tf.keras.layers.BatchNormalization(dtype=dtype))\n",
    "    model.add(Activation(activations.tanh,dtype=dtype))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2DTranspose(filters=3,kernel_size=(5,5),strides=(2,2),padding='same',dtype=dtype))\n",
    "    model.add(tf.keras.layers.Activation('tanh',dtype=dtype))\n",
    "    #model.add(tf.keras.layers.Add(1.0))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adf22b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to provide the discriminator of the GAN\n",
    "def discriminator_model(dropout =.1, filters_list = [2**7,2**8,2**9]):\n",
    "    drop_out_list = [dropout,\n",
    "                     dropout,\n",
    "                     dropout]\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=(input_height,input_width,input_channels)))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=filters_list[0],kernel_size=(5,5),strides=(2,2),padding='same'))\n",
    "    model.add(tf.keras.layers.Dropout(drop_out_list[0]))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=filters_list[1],kernel_size=(5,5),strides=(2,2),padding='same'))\n",
    "    model.add(tf.keras.layers.Dropout(drop_out_list[1]))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=filters_list[2],kernel_size=(5,5),strides=(2,2),padding='same'))\n",
    "    model.add(tf.keras.layers.Dropout(drop_out_list[2]))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(units=1))\n",
    "    model.add(tf.keras.layers.Activation('sigmoid'))\n",
    "    return model\n",
    "\n",
    "def discriminator_model_dense_working(dropout =.1):\n",
    "    drop_out_list = [.5,.4,.33]\n",
    "    filters_list = [64]\n",
    "    model = tf.keras.Sequential()    \n",
    "    model.add(tf.keras.layers.BatchNormalization(input_shape=(input_height,input_width,input_channels)))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=filters_list[0],kernel_size=(5,5),strides=(2,2),padding='same'))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(units=8))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Dense(units=4))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Dense(units=2))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(units=1))\n",
    "    model.add(tf.keras.layers.Activation('sigmoid'))\n",
    "    return model\n",
    "\n",
    "# function to provide the discriminator of the GAN\n",
    "def discriminator_model(dropout =.1, filters_list = [2**7,2**8,2**9]):\n",
    "    drop_out_list = [.5,.4,.33]\n",
    "    filters_list = [8,16,32]\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=(input_height,input_width,input_channels)))    \n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Activation('selu'))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(filters=1,kernel_size=(5,5),strides=(1,1),padding='same'))\n",
    "    #model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Activation('selu'))\n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(units=200))\n",
    "    #model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Activation('selu'))\n",
    "             \n",
    "    model.add(tf.keras.layers.Dense(units=200))\n",
    "    #model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Activation('selu'))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(units=200))\n",
    "    #model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Activation('selu'))\n",
    "\n",
    "    \n",
    "    #for i in [512,512,256,256,128,128,64,64,32,32,16,16,8,8,4,4,2,2]:\n",
    "    #for i in [512,512,128,128,64,64,16,16,4,4,2,2]:\n",
    "    for i in list(range(202,2,-20)):\n",
    "        model.add(tf.keras.layers.Dense(units=i))\n",
    "        #model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Activation('selu'))\n",
    "\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(units=1))\n",
    "    model.add(tf.keras.layers.Activation('sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d05ef9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4c96096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss,real_loss,fake_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7e2aa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    # generator wants to fool the discriminator\n",
    "    # so we must define a loss function for the generator\n",
    "\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93977650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selective discriminator on 5,6,7,8,9\n",
    "# no selective discriminator on 1,2,3,4,\n",
    "\n",
    "# tanh on 6,7,8,9\n",
    "\n",
    "scale = 4 # 1,2\n",
    "scale = 20 #4,5,6\n",
    "scale = 15 #7\n",
    "scale = 10 #3,8,9\n",
    "scale = 4 #X\n",
    "\n",
    "disc_dropout_scale =  1 # 1,2,3,4,5,6,7,8,\n",
    "disc_dropout_scale = .5 # 9\n",
    "disc_dropout_scale = .1 # X\n",
    "\n",
    "orig_gen = 4e-5\n",
    "orig_dis = 5e-7\n",
    "\n",
    "mod_gen = 4e-5+4e-5 #1,2,3,4,5,6,7,8,9\n",
    "mod_dis = 5e-7+0    #1,2,3,4,5,6,7,8,9\n",
    "mod_dis = .9e-7+0    #X\n",
    "\n",
    "ratio = (4e-5/.5e-6)\n",
    "ratio = ((orig_gen)/\n",
    "         (orig_dis))\n",
    "\n",
    "gen_lr = mod_gen*scale\n",
    "disc_lr = mod_dis*scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b4e6cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generator_model(output_width=input_width,\n",
    "                            output_height=input_height,\n",
    "                            output_channels=input_channels,\n",
    "                            latent_dim= 512)\n",
    "print(generator.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b6dc16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = discriminator_model(dropout=disc_dropout_scale)\n",
    "print(discriminator.summary())\n",
    "\n",
    "#discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=.000002,beta_1=.5)\n",
    "#discriminator.compile(loss='binary_crossentropy',optimizer=discriminator_optimizer,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca602609",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# a= b/c\n",
    "# b= ac\n",
    "# c=b/a\n",
    "print(gen_lr,disc_lr,ratio,gen_lr/disc_lr)\n",
    "generator_optimizer     = tf.keras.optimizers.Adam(gen_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c886d7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(disc_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "947cd835",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)\n",
    "\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
    "#run_exp.noise = tf.random.normal([use_batch_size, exp.latent_dim])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be556458",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "  predictions = model(test_input, training=False).numpy()\n",
    "  print(predictions.shape,predictions.dtype)\n",
    "  fig = plt.figure(figsize=(8,2),dpi=160)\n",
    "  #fig = plt.figure()\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(2, 8, i+1)\n",
    "      plt.imshow((predictions[i, :, :, :] * 255).astype(np.uint8))\n",
    "      plt.axis('off')  \n",
    "  plt.show()\n",
    "\n",
    "def save_images(image = None,noise= None,generator= None,epoch=None):\n",
    "    if not image:\n",
    "        #print(\"noise: \",noise)\n",
    "        predictions = generator(noise, training=False).numpy()\n",
    "        #print(predictions.shape,predictions.dtype)\n",
    "\n",
    "        # create hstack the images\n",
    "        all = [(x,y) for x in range(8) for y in range(2)]\n",
    "\n",
    "        for i,t in enumerate(all):\n",
    "            x,y = t\n",
    "            display.np_img[\n",
    "                y*input_height:(y+1)*input_height,\n",
    "                x*input_width:(x+1)*input_width,\n",
    "                :] = predictions[i,:,:,:]\n",
    "        img = Image.fromarray(((display.np_img+1)* 128).astype(np.uint8))\n",
    "    \n",
    "    else:\n",
    "        img = image\n",
    "    \n",
    "    img.save('predictions_at_epoch_{:04d}.png'.format(epoch+1))\n",
    "    return img\n",
    "\n",
    "def display(models:[],\n",
    "            epoch,\n",
    "            epoch_max,\n",
    "            results_history,\n",
    "            tot_batches,\n",
    "            noise,\n",
    "            plot_graphs=True,\n",
    "            plot_images=True,\n",
    "            ):\n",
    "    if \"np_img\" not in dir():\n",
    "        display.np_img= np.zeros(\n",
    "            (\n",
    "                input_height*2,\n",
    "                input_width*8,\n",
    "                input_channels\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    # Produce images for the GIF as you go\n",
    "    ipydisplay.clear_output(wait=False)\n",
    "    if plot_images:\n",
    "        t = save_images(noise=noise,generator =models[0],epoch=epoch)\n",
    "        ipydisplay.display(t)\n",
    "        \n",
    "    if plot_graphs:\n",
    "        clrs = plt.cm.get_cmap(\"hsv\", len(results_history))\n",
    "        # make the plot wide\n",
    "        plt.figure(figsize=(8,2),dpi=160)\n",
    "        for i, key in enumerate(results_history):\n",
    "            if ('epoch' not in key) & (key != \"fooled\")&('_lr' not in key)&('scale' not in key):\n",
    "                clr = clrs(i)\n",
    "                clr=[\n",
    "                     max(clr[0]-(i/255),0),\n",
    "                     max(clr[1]-(i/255),0),\n",
    "                     max(clr[2]-(i/255),0)\n",
    "                 ]\n",
    "                plot_history_clipped = prepare_series_for_graph(results_history[key][-1000:], clip=True, smooth=False)\n",
    "                plot_history_clipped_smoothed = prepare_series_for_graph(results_history[key][-1000:], clip=True, smooth=True)\n",
    "\n",
    "                plt.plot(plot_history_clipped, linewidth=.5, alpha=.35, color=clr)\n",
    "                # use a thinner line for the smoothed results_history\n",
    "                plt.plot(plot_history_clipped_smoothed, label=key + ' smoothed', linewidth=1,alpha=.8, color=clr)\n",
    "\n",
    "        plt.legend(loc= 'upper left')\n",
    "        plt.show()\n",
    "\n",
    "        #plot_history_clipped = prepare_series_for_graph(results_history[\"fooled\"][-1000:], clip=True, smooth=False)\n",
    "        #plot_history_clipped_smoothed = prepare_series_for_graph(results_history[\"fooled\"][-1000:], clip=True, smooth=True)\n",
    "\n",
    "        plt.figure(figsize=(8,2),dpi=160)\n",
    "        plt.rcParams.update({'font.size': 5})\n",
    "        #plt.plot(plot_history_clipped, linewidth=.5, alpha=.33)\n",
    "        # use a thinner line for the smoothed results_history\n",
    "\n",
    "        #plt.yscale('log')\n",
    "        #plt.ylim(0,20)\n",
    "        #plt.plot(results_history[\"fooled_per\"], label=\"fooled_per\", linewidth=1,alpha =.80)\n",
    "        plt.plot(results_history[\"g_grad_scale_accum\"][-1000:], label=\"g_grad_scale_accum\" ,  linewidth=1,alpha =1)\n",
    "        plt.plot(results_history[\"d_grad_scale_accum\"][-1000:], label=\"d_grad_scale_accum\" ,  linewidth=1,alpha =1)\n",
    "        plt.plot(results_history[\"equilibrium_scale\"][-1000:], label=\"equilibrium_scale\" ,  linewidth=1,alpha =1)\n",
    "        plt.plot(results_history[\"grad_scale_power\"][-1000:], label=\"grad_scale_power\" ,  linewidth=1,alpha =1)\n",
    "        plt.legend(loc= 'upper left')\n",
    "        plt.show()\n",
    "        \n",
    "    # Save the model every 15 epochs\n",
    "    if (epoch + 1) % 15 == 0:\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    # print the results average using f strings\n",
    "\n",
    "    def mean(lst:[]):\n",
    "        try:\n",
    "            return sum(lst) / len(lst)\n",
    "        except tf.errors.InvalidArgumentError as e:\n",
    "            return tf.reduce_mean(tf.cast(lst,tf.float32))\n",
    "        \n",
    "    if \"current_epoch\" in results_history:\n",
    "        results_history[\"last_epoch\"] = results_history['current_epoch']\n",
    "\n",
    "    results_history['current_epoch']={ k+\"_mean\":mean(v[-100:]) for k,v in results_history.items() if type(v)==list}\n",
    "\n",
    "    print(f\"gen_loss: {results_history['current_epoch']['gen_loss_mean']}\")\n",
    "    print(f\"disc_loss: {results_history['current_epoch']['disc_loss_mean']}\")\n",
    "    print(f\"fooled_per: {results_history['current_epoch']['fooled_per_mean']}\")\n",
    "    print(f\"g_grad_scale_accum: {results_history['current_epoch']['g_grad_scale_accum_mean']}\")\n",
    "    print(f\"d_grad_scale_accum: {results_history['current_epoch']['d_grad_scale_accum_mean']}\")\n",
    "    print(f\"Epoch: {epoch + 1} / {epoch_max}\")\n",
    "    print(f\"Batches: {tot_batches}\")\n",
    "    print(f\"D Time: {time.time() - start} for images: {plot_images} and graphs: {plot_graphs}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c788b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maintain_history(_results_history,_results):\n",
    "    if len(_results_history)==1:\n",
    "        #_results_history = dict(_results)\n",
    "        for key in _results:\n",
    "            if key != 'epoch':\n",
    "                _results_history[key] = []\n",
    "    else:\n",
    "        # append results to results_history\n",
    "        for key in _results_history:\n",
    "            if 'epoch' not in key:\n",
    "                _results_history[key].append(_results[key])\n",
    "\n",
    "\n",
    "    return _results_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c158d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(generator,\n",
    "          discriminator,\n",
    "          dataset,\n",
    "          epochs,\n",
    "          start_epoch,\n",
    "          results_history,\n",
    "          d_grad_scale_accum,\n",
    "          g_grad_scale_accum,\n",
    "          grad_scale_power,\n",
    "          example_noise,\n",
    "          use_grad_scale):\n",
    "\n",
    "    if start_epoch == -1 and os.path.exists('results_history.pkl'):\n",
    "        try:\n",
    "            print(\"Loading results_history from file\")\n",
    "            results_history = pickle.load(open('results_history.pkl', 'rb'))\n",
    "            start_epoch = results_history['epoch']\n",
    "        except:\n",
    "            print(\"Could not load results_history.pkl\")\n",
    "            start_epoch = 0\n",
    "    elif start_epoch ==-1:\n",
    "        results_history={'epoch': 0,\n",
    "                    'disc_loss': [],\n",
    "                    'real_loss': [],\n",
    "                    'fake_loss': [],\n",
    "                    'gen_loss': [],\n",
    "                    'fooled': [],\n",
    "                    'g_grad_scale_accum': [],\n",
    "                    'd_grad_scale_accum': [],\n",
    "                    'fooled_per': [],\n",
    "                    'equilibrium_scale': [],\n",
    "                    'grad_scale_power': []}\n",
    "    elif (start_epoch > 0):\n",
    "        start_epoch= results_history['epoch']\n",
    "    \n",
    "    #d_grad_scale_accum = tf.Variable(1.0)\n",
    "    \n",
    "    print(\"Starting from epoch: \", start_epoch)\n",
    "\n",
    "    \n",
    "    for epoch in range(start_epoch,start_epoch+epochs):\n",
    "        results_history[\"epoch\"] = epoch\n",
    "\n",
    "        start = time.time()\n",
    "        tot_batches = 0\n",
    "        # train on each batch and generates/updates results history\n",
    "\n",
    "        mstart = time.time()\n",
    "        ts_total_time=0.0\n",
    "        print(\"Starting epoch {}\".format(epoch+1))\n",
    "        for image_batch in dataset:\n",
    "            # accumulate the results of each train_step\n",
    "            ts_start_time = time.time()\n",
    "            if'current_epoch' in results_history:\n",
    "                train._results = train_step(generator,discriminator,image_batch,\n",
    "                                            selective_discriminator_training=False,\n",
    "                                            hist=results_history['current_epoch'],\n",
    "                                            d_grad_scale_accum= d_grad_scale_accum,\n",
    "                                            g_grad_scale_accum=g_grad_scale_accum,\n",
    "                                            grad_scale_power=grad_scale_power,\n",
    "                                            use_grad_scale=use_grad_scale)\n",
    "            else:\n",
    "                train._results = train_step(generator,discriminator,image_batch,\n",
    "                                            selective_discriminator_training=False,\n",
    "                                            hist=None,\n",
    "                                            d_grad_scale_accum= d_grad_scale_accum,\n",
    "                                            g_grad_scale_accum=g_grad_scale_accum,\n",
    "                                            grad_scale_power=grad_scale_power,\n",
    "                                            use_grad_scale=use_grad_scale)\n",
    "\n",
    "            ts_time = time.time() - ts_start_time\n",
    "            ts_total_time += ts_time\n",
    "            #train._results['gen_lr'] = [generator_optimizer.learning_rate.numpy()]*train._results['batch_size']\n",
    "            #train._results['disc_lr'] = discriminator_optimizer.learning_rate.numpy()\n",
    "            results_history=maintain_history(results_history,train._results)\n",
    "            tot_batches += 1\n",
    "\n",
    "\n",
    "        mend = time.time()\n",
    "        print(f\"Time for maintaining results: {mend - mstart-ts_total_time}\")\n",
    "        \n",
    "        \n",
    "        if (epoch % 10 == 0) or (epoch == 1):\n",
    "            display.fig = plt.figure(figsize=(8,1.9),dpi=160)\n",
    "            display([generator,discriminator],\n",
    "                    epoch,\n",
    "                    start_epoch+epochs,                    \n",
    "                    results_history,\n",
    "                    tot_batches,\n",
    "                    noise=example_noise)\n",
    "            # save results_history to disk as results_history.pkl\n",
    "            with open('results_history.pkl', 'wb') as f:\n",
    "                pickle.dump(results_history, f)\n",
    "        save_images(noise=example_noise,generator =generator,epoch=epoch)\n",
    "        print(\"Time for epoch: \", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f583d5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_series_for_graph(np_arr,smooth = True,clip =True):\n",
    "    # print performance for this function\n",
    "    s_time = time.time()\n",
    "    out = np.array(np_arr)\n",
    "\n",
    "    # smooth the results_history\n",
    "    if smooth:\n",
    "        out  = np.convolve(out, np.ones((10,))/10, mode='valid')\n",
    "    # clip the results_history\n",
    "    if clip:\n",
    "        c=np.percentile(out, 95)\n",
    "        out = out[out < c]\n",
    "    e_time = time.time()\n",
    "    #print(f\"Time for smooth: {smooth} and clip: {clip} was {e_time - s_time}\")\n",
    "\n",
    "    return out.astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d1040a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(generator,discriminator,\n",
    "               images,               \n",
    "               selective_discriminator_training,               \n",
    "               hist,\n",
    "               d_grad_scale_accum,\n",
    "               g_grad_scale_accum,\n",
    "               grad_scale_power,\n",
    "               use_grad_scale:bool):\n",
    "    noise = tf.random.normal((len(images),\n",
    "                              generator.layers[0].input.shape[1]\n",
    "                             ))\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        # Generate noise to feed to the generator and get generated images.\n",
    "        generated_images = generator(noise, training = generator.trainable)\n",
    "        # call the discriminator with the real images and get its guesses\n",
    "        real_output = discriminator(images, training = discriminator.trainable )\n",
    "        # call the discriminator with the generated images and get the output\n",
    "        fake_output = discriminator(generated_images, training = discriminator.trainable)\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss,real_loss,fake_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    # track the number of fake images that fooled the discriminator where >.5\n",
    "    fooled_per = tf.divide(tf.math.count_nonzero(fake_output > .5),BATCH_SIZE,\"fooled_percent_calc\")\n",
    "    fooled = tf.math.count_nonzero(fake_output > .5)\n",
    "\n",
    "    # Calculate the gradients for generator and discriminator.\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)    \n",
    "    report_d_grad_scale=1.0\n",
    "    report_g_grad_scale=1.0\n",
    "    equilibrium_scale=1.0\n",
    "    #equilibrium_scale =tf.Variable(0,dtype=tf.float32)\n",
    "    if use_grad_scale:\n",
    "        #### Gen\n",
    "        g_grad_scale = tf.subtract(            \n",
    "            tf.cast(fooled_per ,dtype=tf.float32),\n",
    "            tf.constant(.5, dtype=tf.float32)\n",
    "        )\n",
    "        g_grad_scale = g_grad_scale * -2\n",
    "        # range is -1 to +1, +1 when 100% of samples in batch DO NOT fool the discriminator\n",
    "        g_grad_scale = g_grad_scale * grad_scale_power\n",
    "        \n",
    "        \n",
    "        \n",
    "        #### Disc\n",
    "        d_grad_scale = tf.subtract(            \n",
    "            tf.cast(fooled_per ,dtype=tf.float32),\n",
    "            tf.constant(.5, dtype=tf.float32)\n",
    "        )\n",
    "        d_grad_scale = d_grad_scale * 2\n",
    "        # range is -1 to +1, +1 when 100% of samples in batch fool the discriminator\n",
    "        d_grad_scale = d_grad_scale * grad_scale_power\n",
    "        \n",
    "        \n",
    "        #equilibrium_scale=tf.cast((-((5*fooled_per)-2.5)**2+4)/6, dtype=tf.float32)*grad_scale_power\n",
    "        equilibrium_scale=tf.cast((-((5.0*fooled_per)-2.0)**2.0+4.0)/4.0, dtype=tf.float32)*grad_scale_power\n",
    "        #equilibrium_scale =(0.1(1/((fooled_per*4)âˆ’2)))**2\n",
    "        \n",
    "        d_grad_scale_accum.assign_add(d_grad_scale+equilibrium_scale)\n",
    "        d_grad_scale_accum = tf.maximum(d_grad_scale_accum,-0.99)       \n",
    "        d_grad_scale_tot = tf.constant(1.0, dtype=tf.float32) + d_grad_scale_accum\n",
    "        \n",
    "\n",
    "        g_grad_scale_accum.assign_add(g_grad_scale+equilibrium_scale)        \n",
    "        g_grad_scale_accum = tf.maximum(g_grad_scale_accum,-0.99)\n",
    "        g_grad_scale_tot = tf.cast(\n",
    "            tf.add(tf.constant(1.0, dtype=tf.float32),g_grad_scale_accum),\n",
    "            tf.float32)\n",
    "        \n",
    "        gradients_of_generator= [tf.math.scalar_mul(g_grad_scale_tot, grad) for grad in gradients_of_generator]\n",
    "        gradients_of_discriminator= [tf.math.scalar_mul(d_grad_scale_tot,grad) for grad in gradients_of_discriminator]\n",
    "                \n",
    "        report_d_grad_scale=d_grad_scale_accum\n",
    "        report_g_grad_scale=g_grad_scale_accum    \n",
    "       \n",
    "        \n",
    "        \n",
    "    # Apply gradients to the optimizer.\n",
    "    generator.optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    #generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "\n",
    "    if selective_discriminator_training:\n",
    "        if gen_loss < 1:\n",
    "            #discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "            discriminator.optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    else:\n",
    "        discriminator.optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "        #discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    # return a dictionary of disc_loss, real_loss, fake_loss, gen_loss,fooled\n",
    "    disc_lr = tf.keras.backend.get_value(discriminator.optimizer.learning_rate)\n",
    "    gen_lr = tf.keras.backend.get_value(generator.optimizer.learning_rate)\n",
    "    return {'disc_loss': disc_loss,\n",
    "            'real_loss': real_loss,\n",
    "            'fake_loss': fake_loss,\n",
    "            'gen_loss': gen_loss,\n",
    "            'fooled': fooled,\n",
    "            'g_grad_scale_accum': report_g_grad_scale,\n",
    "            'd_grad_scale_accum': report_d_grad_scale,\n",
    "            'fooled_per':fooled_per,\n",
    "            'equilibrium_scale':equilibrium_scale,\n",
    "            'grad_scale_power':grad_scale_power}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d02e5512",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d_grad_scale_accum = tf.Variable(0.0)\n",
    "g_grad_scale_accum = tf.Variable(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a27e513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class experiment:\n",
    "    name:str\n",
    "    generator:None\n",
    "    discriminator:None\n",
    "    generator_src=None\n",
    "    discriminator_src=None\n",
    "    batch_size:int = 64\n",
    "    train_shape:()=(0,0)\n",
    "    example_noise = None\n",
    "    g_lr:float=1e-6\n",
    "    d_lr:float=1e-6\n",
    "    start_gen_gsa:float=0.0\n",
    "    start_disc_gsa:float=0.0\n",
    "    grad_power:float=.01    \n",
    "    epochs:int = 2000\n",
    "    results_history=None\n",
    "    use_grad_scale:bool=True\n",
    "    latent_dim:int =100\n",
    "    dropout:float=0.0\n",
    "    disc_filters_list:list=None\n",
    "    gen_filters_list:list=None\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0cf94297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Tasha\\Desktop\\Jupyter-GPU\\data\\nasa_planet_gan\\wandb\\run-20220724_023515-f474kena</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/tashaskyup/nasa_planet_gan/runs/f474kena\" target=\"_blank\">likely-gorge-10</a></strong> to <a href=\"https://wandb.ai/tashaskyup/nasa_planet_gan\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/tashaskyup/nasa_planet_gan/sweeps/ymtfpo8o\" target=\"_blank\">https://wandb.ai/tashaskyup/nasa_planet_gan/sweeps/ymtfpo8o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import wandb\n",
    "def wnb():\n",
    "    with wandb.init() as run:\n",
    "        config = wandb.config\n",
    "        ex= experiment(f'wnb',\n",
    "                  generator=None,\n",
    "                  discriminator=None,\n",
    "                  g_lr = config[\"g_lr\"],\n",
    "                  d_lr = config[\"d_lr\"],\n",
    "                  start_gen_gsa=0.0,\n",
    "                  start_disc_gsa=0.0,\n",
    "                  grad_power=.000001,\n",
    "                  batch_size=[config[\"batch_size\"]],\n",
    "                  epochs=100,\n",
    "                  latent_dim = 300,\n",
    "                  dropout=0.0,\n",
    "                  use_grad_scale= False,\n",
    "                  disc_filters_list = None,\n",
    "                  gen_filters_list = [64+32,32+16] \n",
    "                  )\n",
    "        print (config)\n",
    "        run_exp(ex)\n",
    "\n",
    "\n",
    "count = 5 # number of runs to execute\n",
    "wandb.agent(\"ymtfpo8o\", function=wnb, count=30)\n",
    "\n",
    "#print(exp3.results_history,exp3.batch_size)\n",
    "#run_exp(exp3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
